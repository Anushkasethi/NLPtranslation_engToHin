{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kmsy71e2T7mJ"
      },
      "outputs": [],
      "source": [
        "# !pip install indic-nlp-library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qCkwM2-qUAzx"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101, 31178,   117, 14796, 10301, 13028,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[  101,   852, 18187, 15399, 58871, 10532,   875, 53809, 18869, 25695,\n",
            "         62010, 10569,   146, 10392, 19353,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "['[CLS]', 'आ', '##प', '##का', 'हिंदी', 'में', 'ट', '##ेक', '##्स', '##्ट', 'यहां', 'है', 'I', 'am', 'here', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize input sentences\n",
        "input_sentence = \"Hello, how are you?\"\n",
        "tokens = tokenizer(input_sentence, return_tensors=\"pt\")\n",
        "print(tokens)\n",
        "input_sentence_hin = \"आपका हिंदी में टेक्स्ट यहां है I am here\"\n",
        "tokens_hin = tokenizer(input_sentence_hin, return_tensors=\"pt\")\n",
        "print(tokens_hin)\n",
        "print(tokenizer.convert_ids_to_tokens(tokens_hin['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SBqfFE4DyxKP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max_sequence length: 54\n",
            "['do not open that.', 'वह मत खोलिए।']\n",
            "['mary prided herself on her beauty.', 'मेरी को अपनी सुंदरता पर बहुत नाज़ था।']\n",
            "[\"english is the world's language.\", 'अंग्रेज़ी वैश्विक भाषा है।']\n",
            "['i forgot.', 'मैं भूल गया।']\n",
            "['i would not do that to anybody.', 'मैं ऐसा किसी के साथ नहीं करुंगी।']\n"
          ]
        }
      ],
      "source": [
        "with open(r\"hin.txt\", 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "hindi_vocab_itos = {\n",
        "    0: tokenizer.unk_token,\n",
        "    1: tokenizer.pad_token,\n",
        "}\n",
        "\n",
        "english_vocab_itos = {\n",
        "    0: tokenizer.unk_token,\n",
        "    1: tokenizer.pad_token\n",
        "}\n",
        "\n",
        "hindi_vocab_stoi = {}\n",
        "english_vocab_stoi = {}\n",
        "\n",
        "\n",
        "all_hindi_tokens = []\n",
        "all_english_tokens = []\n",
        "text_pairs = []\n",
        "max_seq_len = 0\n",
        "for line in lines:\n",
        "    eng, hin, rubbish = line.split(\"\\t\")\n",
        "    eng = ' '.join([contraction_mapping[t.lower()] if t.lower() in contraction_mapping else t.lower() for t in eng.split()])\n",
        "    text_pairs.append([eng, hin])\n",
        "    max_seq_len = max(max_seq_len, len(tokenizer.tokenize(hin)), len(tokenizer.tokenize(eng)))\n",
        "    all_hindi_tokens += tokenizer.encode(hin)\n",
        "    all_english_tokens += tokenizer.encode(eng)\n",
        "\n",
        "hindi_tokens_set = set(all_hindi_tokens)\n",
        "english_tokens_set = set(all_english_tokens)\n",
        "\n",
        "for i, token in enumerate(hindi_tokens_set, start = 2):\n",
        "    hindi_vocab_itos[i] = tokenizer.convert_ids_to_tokens([token])[0]\n",
        "\n",
        "for i, token in enumerate(english_tokens_set, start = 2):\n",
        "    english_vocab_itos[i] = tokenizer.convert_ids_to_tokens([token])[0]\n",
        "\n",
        "\n",
        "for id, string in hindi_vocab_itos.items():\n",
        "    hindi_vocab_stoi[string] = id\n",
        "\n",
        "for id, string in english_vocab_itos.items():\n",
        "    english_vocab_stoi[string] = id\n",
        "\n",
        "max_seq_len += 2 #to account for CLS and SEP tokens\n",
        "random.shuffle(text_pairs)\n",
        "print('Max_sequence length:', max_seq_len)\n",
        "for i in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aW3uOoUznUH",
        "outputId": "c8c74d12-709f-4485-fb32-376c96a3b2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2979 total pairs\n",
            "2085 training pairs\n",
            "447 validation pairs\n",
            "447 test pairs\n"
          ]
        }
      ],
      "source": [
        "num_train = int(0.70 * len(text_pairs))\n",
        "num_val = int((len(text_pairs) - num_train) / 2)\n",
        "train_pairs = text_pairs[:num_train]\n",
        "val_pairs = text_pairs[num_train : num_train + num_val]\n",
        "test_pairs = text_pairs[num_train + num_val :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iscn8UgpuGA9",
        "outputId": "e6243e0e-878f-43d2-aa07-e2a21d278e71"
      },
      "outputs": [],
      "source": [
        "train_eng = []\n",
        "for pair in train_pairs:\n",
        "    train_eng.append(pair[0])\n",
        "\n",
        "train_hin = []\n",
        "for pair in train_pairs:\n",
        "    train_hin.append(pair[1])\n",
        "\n",
        "target_hin = []\n",
        "for pair in train_pairs:\n",
        "    target_hin.append(pair[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sVEYeD1JWuR2"
      },
      "outputs": [],
      "source": [
        "# Add padding to the tokenized sequences\n",
        "padded_tokens = tokenizer(train_eng, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n",
        "# Add padding to the tokenized sequences\n",
        "padded_tokens_hin = tokenizer(train_hin, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n",
        "\n",
        "padded_tokens_target = tokenizer(target_hin, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 61694, 10133, 15127, 11324, 10124, 102], [101, 12976, 10124, 10105, 35660, 18745, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
            "['[CLS]', 'what', 'is', 'the', 'weather', 'today', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "input_sentence = [\"hello my name is\", \"what is the weather today\"]\n",
        "tokens = tokenizer(input_sentence)\n",
        "print(tokens)\n",
        "print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][1]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n"
          ]
        }
      ],
      "source": [
        "print(len(padded_tokens[\"input_ids\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh7I1YNzYa13",
        "outputId": "f195ae96-3b70-4977-842a-8a218b35f2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  101,   177, 10149, 10472, 34481, 13172, 22111,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "input_ids = padded_tokens[\"input_ids\"]\n",
        "input_ids_hin = padded_tokens_hin[\"input_ids\"]\n",
        "target_ids = padded_tokens_target[\"input_ids\"]\n",
        "attention_mask = padded_tokens[\"attention_mask\"]\n",
        "attention_mask_hin = padded_tokens_hin[\"attention_mask\"]\n",
        "# print(attention_mask_hin, attention_mask, input_ids, input_ids_hin)\n",
        "print(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n",
            "[CLS] my name is [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "english_tokens = tokenizer(\"my name is\", padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n",
        "print(len(english_tokens['input_ids'][0]))\n",
        "english_text = tokenizer.decode(english_tokens['input_ids'][0])\n",
        "print(english_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMWb7qOE1SG8",
        "outputId": "69f527b6-88ac-4b05-e596-53eec96dd9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2085, 54])\n",
            "torch.Size([2085, 54])\n"
          ]
        }
      ],
      "source": [
        "print(input_ids_hin.shape)\n",
        "print(input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QgRKa3QkEETc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Example usage\n",
        "english_vocab_size = len(english_vocab_itos)  # Update with your actual vocabulary size\n",
        "hindi_vocab_size = len(hindi_vocab_itos)\n",
        "embed_dim = 512\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "batch_size = 8\n",
        "num_blocks = 2\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.tril(torch.ones(num_heads, size, size)).type(torch.int)\n",
        "    return mask\n",
        "\n",
        "# Assume you have a custom dataset class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset_pairs, tokenizer, max_seq_len):\n",
        "        self.dataset_pairs = dataset_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        english = self.dataset_pairs[index][0]\n",
        "        hindi = self.dataset_pairs[index][1]\n",
        "\n",
        "        english = self.tokenizer.tokenize(english)\n",
        "        hindi = self.tokenizer.tokenize(hindi)\n",
        "        \n",
        "        english_ids = [english_vocab_stoi[\"[PAD]\"]]*self.max_seq_len\n",
        "        hindi_ids = [hindi_vocab_stoi[\"[PAD]\"]]*self.max_seq_len\n",
        "        \n",
        "        for i, string in enumerate(english):\n",
        "            english_ids[i] = english_vocab_stoi[string]\n",
        "\n",
        "        for i, string in enumerate(hindi):\n",
        "            hindi_ids[i] = hindi_vocab_stoi[string]\n",
        "\n",
        "        english_ids = torch.tensor(english_ids)\n",
        "        hindi_ids = torch.tensor(hindi_ids)\n",
        "        decoder_input = hindi_ids.clone()\n",
        "        decoder_input[decoder_input == hindi_vocab_stoi[\"[SEP]\"]] = hindi_vocab_stoi[\"[PAD]\"]\n",
        "        target = hindi_ids.clone()\n",
        "        target = torch.cat((target[1:], torch.tensor([hindi_vocab_stoi[\"[PAD]\"]])))\n",
        "\n",
        "        encoder_mask = torch.tensor([elem.item() != english_vocab_stoi[\"[PAD]\"] for elem in english_ids]).float()\n",
        "        decoder_mask = torch.tensor([elem.item() != hindi_vocab_stoi[\"[PAD]\"] for elem in decoder_input]).unsqueeze(0) & causal_mask(self.max_seq_len)\n",
        "        return {\n",
        "            'encoder_input': english_ids, \n",
        "            'decoder_input': decoder_input, \n",
        "            'target': target, \n",
        "            'encoder_mask': encoder_mask, \n",
        "            'decoder_mask': decoder_mask.to(torch.bool),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Create a dataset and data loader\n",
        "translation_dataset = TranslationDataset(dataset_pairs=train_pairs, tokenizer=tokenizer, max_seq_len=max_seq_len)\n",
        "train_data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2moq1HISDsWp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_length, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "      return x + self.encoding[:, :x.size(1)].detach().to(self.device)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, max_seq_len):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.multi_head_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(max_seq_len * d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, max_seq_len * d_model),\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, src, encoder_mask):\n",
        "      \n",
        "        out1, _ = self.multi_head_attention(src, src, src, key_padding_mask=encoder_mask)\n",
        "        src = src + out1  \n",
        "        src = self.norm1(src)\n",
        "        flat = torch.flatten(src, start_dim=1)\n",
        "        out2 = self.feedforward(flat)\n",
        "        out2 = out2.view(src.shape)\n",
        "        src = src + out2  \n",
        "        src = self.norm1(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, max_seq_len, num_blocks):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncoding(max_length=max_seq_len, d_model=d_model)\n",
        "        self.encoder_blocks = nn.ModuleList([EncoderBlock(d_model, num_heads, max_seq_len) for _ in range(num_blocks)])\n",
        "        \n",
        "    def forward(self, src, encoder_mask):\n",
        "\n",
        "        src = self.embedding_layer(src)\n",
        "        src = self.position_encoding(src)\n",
        "        \n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            src = encoder_block(src, encoder_mask)\n",
        "\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, max_seq_len, batch_size):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, num_heads=num_heads, batch_first = True)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model*max_seq_len, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, d_model*max_seq_len)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.batch_size=batch_size\n",
        "        self.num_heads = num_heads\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, embedded_hin, decoder_mask, encoder_output):\n",
        "      \n",
        "        decoder_mask=decoder_mask.view(-1, self.max_seq_len, self.max_seq_len)\n",
        "        self_attention_output, _ = self.self_attention(embedded_hin, embedded_hin, embedded_hin, attn_mask=decoder_mask)\n",
        "        self_attention_output = self.layer_norm1(embedded_hin + self_attention_output)\n",
        "        multi_out, _ = self.self_attention(encoder_output, encoder_output, self_attention_output, attn_mask=decoder_mask)\n",
        "        multi_out = self.layer_norm1(multi_out + self_attention_output)\n",
        "        flat = torch.flatten(multi_out, start_dim =1)\n",
        "        feedforward_output = self.feedforward(flat)\n",
        "        feedforward_output = feedforward_output.view(multi_out.shape)\n",
        "        decoder_output = self.layer_norm1(multi_out + feedforward_output)\n",
        "        \n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, max_seq_len, batch_size, num_blocks):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(vocab_size, d_model)\n",
        "        self.decoder_block = nn.ModuleList([DecoderBlock(vocab_size, d_model, num_heads, max_seq_len, batch_size) for _ in range(num_blocks)])\n",
        "        \n",
        "    def forward(self, src, decoder_mask, encoder_output):\n",
        "        \n",
        "        src = self.embedding_layer(src)\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        for decoder_block in self.decoder_block:\n",
        "            src = decoder_block(src, decoder_mask, encoder_output)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lQT-e5aB3NuK"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model) -> None:\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, english_vocab_size, hindi_vocab_size, d_model, num_heads, max_seq_len, num_blocks_encoder, num_blocks_decoder, batch_size) -> None:\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(english_vocab_size, d_model, num_heads, max_seq_len=max_seq_len, num_blocks=num_blocks_encoder)\n",
        "        self.decoder = Decoder(hindi_vocab_size, d_model, num_heads, max_seq_len=max_seq_len, batch_size=batch_size, num_blocks=num_blocks_decoder)\n",
        "        self.projection = ProjectionLayer(hindi_vocab_size, d_model)\n",
        "    \n",
        "    def forward(self, batch, device = 'cpu'):\n",
        "\n",
        "        out = self.encoder(batch['encoder_input'].to(device), batch['encoder_mask'].to(device))\n",
        "        out = self.decoder(batch['decoder_input'].to(device), batch['decoder_mask'].to(device), out)\n",
        "        out = self.projection(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUuSSMfy6QAx",
        "outputId": "50ed203c-9bce-46c1-8410-c58d0f7d0572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:18<31:03, 18.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 1.4950737953186035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:38<31:06, 19.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Loss: 1.2518571615219116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/100 [00:56<30:26, 18.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100, Loss: 1.081704020500183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [01:15<29:56, 18.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100, Loss: 1.5252195596694946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 5/100 [01:33<29:31, 18.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100, Loss: 1.087630033493042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [01:52<29:15, 18.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100, Loss: 1.1249574422836304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 7/100 [02:11<29:00, 18.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100, Loss: 1.3743969202041626\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 8/100 [02:30<28:56, 18.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100, Loss: 1.1870533227920532\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 9/100 [02:49<28:38, 18.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100, Loss: 0.7773582339286804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 10/100 [03:07<28:10, 18.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Loss: 1.3084651231765747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [03:25<27:33, 18.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100, Loss: 1.0319551229476929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 12/100 [03:44<27:00, 18.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100, Loss: 1.2776039838790894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 13/100 [04:02<26:44, 18.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/100, Loss: 1.2801989316940308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 14/100 [04:20<26:24, 18.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/100, Loss: 1.0797854661941528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 15/100 [04:39<26:05, 18.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100, Loss: 1.3285444974899292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16/100 [04:57<25:45, 18.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100, Loss: 0.9949142932891846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 17/100 [05:16<25:28, 18.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100, Loss: 1.0045530796051025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 18/100 [05:34<25:07, 18.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100, Loss: 1.530339241027832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 19/100 [05:52<24:48, 18.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100, Loss: 1.1044594049453735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 20/100 [06:10<24:22, 18.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/100, Loss: 1.133337378501892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21/100 [06:29<24:03, 18.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/100, Loss: 1.122960090637207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 22/100 [06:47<23:43, 18.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/100, Loss: 1.0763630867004395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 23/100 [07:05<23:24, 18.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/100, Loss: 0.8910660147666931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 24/100 [07:23<22:57, 18.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100, Loss: 1.0378060340881348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25/100 [07:41<22:29, 18.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/100, Loss: 1.2833051681518555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 26/100 [08:00<22:34, 18.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/100, Loss: 0.9276813268661499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 27/100 [08:19<22:34, 18.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/100, Loss: 1.1363412141799927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 28/100 [08:38<22:27, 18.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/100, Loss: 1.104583978652954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 29/100 [08:57<22:21, 18.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/100, Loss: 0.89387047290802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 30/100 [09:16<22:06, 18.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100, Loss: 1.209564208984375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 31/100 [09:35<21:47, 18.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/100, Loss: 1.0233205556869507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 32/100 [09:54<21:22, 18.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/100, Loss: 0.9159115552902222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 33/100 [10:12<20:59, 18.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/100, Loss: 1.1734423637390137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 34/100 [10:32<20:54, 19.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/100, Loss: 1.3295859098434448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 35/100 [10:51<20:41, 19.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100, Loss: 1.524821400642395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 36/100 [11:10<20:20, 19.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/100, Loss: 1.0756028890609741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 37/100 [11:29<20:02, 19.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100, Loss: 1.0535496473312378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 38/100 [11:48<19:35, 18.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/100, Loss: 1.0953799486160278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 39/100 [12:07<19:10, 18.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100, Loss: 1.3289161920547485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 40/100 [12:25<18:45, 18.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/100, Loss: 1.0590287446975708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [12:44<18:24, 18.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/100, Loss: 0.7571190595626831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 42/100 [13:02<18:01, 18.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/100, Loss: 1.116459846496582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 43/100 [13:21<17:36, 18.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/100, Loss: 1.1811484098434448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [13:39<17:13, 18.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/100, Loss: 1.223030924797058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 45/100 [13:57<16:56, 18.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/100, Loss: 1.0260226726531982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 46/100 [14:16<16:35, 18.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/100, Loss: 1.2381945848464966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 47/100 [14:34<16:21, 18.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/100, Loss: 0.9486432075500488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 48/100 [14:53<16:05, 18.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/100, Loss: 1.2970080375671387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 49/100 [15:12<15:45, 18.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/100, Loss: 0.862549364566803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 50/100 [15:31<15:34, 18.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/100, Loss: 0.9206745624542236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 51/100 [15:50<15:21, 18.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51/100, Loss: 0.7208049297332764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 52/100 [16:09<15:09, 18.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52/100, Loss: 1.0780538320541382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 53/100 [16:28<14:54, 19.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53/100, Loss: 1.0832610130310059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 54/100 [16:46<14:20, 18.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54/100, Loss: 1.1265733242034912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [17:04<13:51, 18.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/100, Loss: 0.9983500242233276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 56/100 [17:22<13:23, 18.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56/100, Loss: 1.1652750968933105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 57/100 [17:40<13:01, 18.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57/100, Loss: 1.0769610404968262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 58/100 [17:58<12:38, 18.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/100, Loss: 1.0065721273422241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 59/100 [18:15<12:17, 18.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59/100, Loss: 0.9180402755737305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [18:33<11:56, 17.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/100, Loss: 1.06667160987854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [18:51<11:36, 17.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61/100, Loss: 0.9752997756004333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 62/100 [19:09<11:20, 17.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62/100, Loss: 1.0505688190460205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 63/100 [19:27<11:02, 17.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63/100, Loss: 0.9450486898422241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 64/100 [19:45<10:44, 17.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64/100, Loss: 1.121248483657837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [20:03<10:27, 17.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65/100, Loss: 0.9675180912017822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 66/100 [20:21<10:13, 18.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66/100, Loss: 1.2854645252227783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 67/100 [20:39<09:58, 18.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67/100, Loss: 1.1595577001571655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 68/100 [20:58<09:44, 18.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68/100, Loss: 0.9123144745826721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 69/100 [21:16<09:25, 18.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69/100, Loss: 1.1933326721191406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 70/100 [21:34<09:08, 18.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70/100, Loss: 1.0693202018737793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 71/100 [21:53<08:50, 18.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71/100, Loss: 1.1079955101013184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 72/100 [22:11<08:31, 18.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72/100, Loss: 1.1809109449386597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [22:29<08:09, 18.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73/100, Loss: 1.4490655660629272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 74/100 [22:47<07:48, 18.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74/100, Loss: 0.9102973937988281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [23:04<07:28, 17.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75/100, Loss: 0.7364130020141602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 76/100 [23:22<07:10, 17.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76/100, Loss: 1.0212128162384033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 77/100 [23:41<06:57, 18.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77/100, Loss: 1.0772887468338013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 78/100 [24:00<06:43, 18.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78/100, Loss: 1.2004708051681519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 79/100 [24:19<06:29, 18.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79/100, Loss: 1.1417198181152344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 80/100 [24:38<06:13, 18.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80/100, Loss: 0.9991678595542908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [24:56<05:53, 18.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81/100, Loss: 0.9045661091804504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 82/100 [25:14<05:30, 18.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82/100, Loss: 0.8693292140960693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 83/100 [25:32<05:09, 18.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83/100, Loss: 1.1541393995285034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 84/100 [25:50<04:49, 18.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84/100, Loss: 0.9447544813156128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 85/100 [26:08<04:30, 18.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85/100, Loss: 0.7450308799743652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [26:25<04:11, 17.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86/100, Loss: 1.1657819747924805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 87/100 [26:43<03:53, 17.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87/100, Loss: 0.8745642304420471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 88/100 [27:01<03:35, 17.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88/100, Loss: 1.149208903312683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 89/100 [27:19<03:17, 17.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89/100, Loss: 0.7350824475288391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 90/100 [27:37<02:59, 17.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90/100, Loss: 0.9980380535125732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 91/100 [27:55<02:41, 17.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91/100, Loss: 1.3997743129730225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 92/100 [28:13<02:23, 17.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92/100, Loss: 1.168387770652771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 93/100 [28:31<02:05, 17.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93/100, Loss: 0.9098108410835266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 94/100 [28:49<01:48, 18.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94/100, Loss: 1.1113200187683105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 95/100 [29:07<01:30, 18.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95/100, Loss: 1.142215371131897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 96/100 [29:25<01:12, 18.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96/100, Loss: 1.1461313962936401\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 97/100 [29:43<00:53, 17.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97/100, Loss: 1.2862664461135864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 98/100 [30:00<00:35, 17.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98/100, Loss: 0.9091157913208008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 99/100 [30:18<00:17, 17.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99/100, Loss: 1.2517513036727905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [30:36<00:00, 18.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/100, Loss: 1.3033874034881592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device', device)\n",
        "\n",
        "# Define a loss function (e.g., cross-entropy) and an optimizer (e.g., Adam)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id).to(device)\n",
        "net = Transformer(english_vocab_size, hindi_vocab_size, d_model, num_heads, max_seq_len=max_seq_len, batch_size=batch_size, num_blocks_encoder=num_blocks, num_blocks_decoder=num_blocks).to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n",
        "net.train()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100  # Adjust as needed\n",
        "# Example of training loop\n",
        "for epoch in tqdm(range(num_epochs)):  # Run for 3 epochs as an example\n",
        "    for batch in train_data_loader:\n",
        "        out = net(batch, device)\n",
        "        # Compute the loss\n",
        "        target = batch['target'].to(device).long()\n",
        "        loss = criterion(out.view(-1, hindi_vocab_size), target.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Print the loss after each epoch\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained models if needed\n",
        "torch.save(net.state_dict(), 'net.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
